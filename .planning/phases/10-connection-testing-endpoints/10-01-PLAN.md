---
phase: 10-connection-testing-endpoints
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/caal/webhooks.py
  - frontend/app/api/setup/test-openai-compatible/route.ts
  - frontend/app/api/setup/test-openrouter/route.ts
autonomous: true

must_haves:
  truths:
    - "POST /setup/test-openai-compatible returns success=true with models array for valid server"
    - "POST /setup/test-openai-compatible returns success=false with error for invalid server/key"
    - "POST /setup/test-openrouter returns success=true with models array for valid API key"
    - "POST /setup/test-openrouter returns success=false with error for invalid API key"
    - "Frontend can call /api/setup/test-openai-compatible and receive proxied response"
    - "Frontend can call /api/setup/test-openrouter and receive proxied response"
  artifacts:
    - path: "src/caal/webhooks.py"
      provides: "TestOpenAICompatibleRequest, TestOpenRouterRequest, test_openai_compatible, test_openrouter"
      contains: "test-openai-compatible"
    - path: "frontend/app/api/setup/test-openai-compatible/route.ts"
      provides: "POST handler proxying to agent"
      min_lines: 15
    - path: "frontend/app/api/setup/test-openrouter/route.ts"
      provides: "POST handler proxying to agent"
      min_lines: 15
  key_links:
    - from: "frontend/app/api/setup/test-openai-compatible/route.ts"
      to: "/setup/test-openai-compatible"
      via: "fetch to WEBHOOK_URL"
      pattern: "WEBHOOK_URL.*test-openai-compatible"
    - from: "frontend/app/api/setup/test-openrouter/route.ts"
      to: "/setup/test-openrouter"
      via: "fetch to WEBHOOK_URL"
      pattern: "WEBHOOK_URL.*test-openrouter"
---

<objective>
Add connection testing endpoints for OpenAI-compatible servers and OpenRouter API.

Purpose: Users need to validate their provider configuration before saving settings. These endpoints test connectivity, validate credentials, and return available models for selection.

Output:
- Two new backend endpoints in webhooks.py (test-openai-compatible, test-openrouter)
- Two frontend proxy routes for browser access
</objective>

<execution_context>
@/Users/mmaudet/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mmaudet/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-connection-testing-endpoints/10-RESEARCH.md

Reference existing patterns:
@src/caal/webhooks.py (see test_ollama, test_groq for exact patterns)
@frontend/app/api/setup/test-groq/route.ts (frontend proxy pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add backend test endpoints for OpenAI-compatible and OpenRouter</name>
  <files>src/caal/webhooks.py</files>
  <action>
Add two new test endpoints following the exact pattern of test_ollama and test_groq.

1. Add Pydantic request models after TestN8nRequest (around line 953):

```python
class TestOpenAICompatibleRequest(BaseModel):
    """Request body for /setup/test-openai-compatible endpoint."""
    base_url: str      # e.g., "http://localhost:8000/v1"
    api_key: str = ""  # Optional - some servers don't need auth


class TestOpenRouterRequest(BaseModel):
    """Request body for /setup/test-openrouter endpoint."""
    api_key: str  # Required - OpenRouter always needs API key
```

2. Add test_openai_compatible endpoint after test_n8n (around line 1195):

```python
@app.post("/setup/test-openai-compatible", response_model=TestConnectionResponse)
async def test_openai_compatible(req: TestOpenAICompatibleRequest) -> TestConnectionResponse:
    """Test OpenAI-compatible server connection and list available models.

    Args:
        req: TestOpenAICompatibleRequest with base URL and optional API key

    Returns:
        TestConnectionResponse with success status and model list
    """
    # Normalize URL - strip trailing slash
    base_url = req.base_url.rstrip("/")

    try:
        headers = {}
        if req.api_key:
            headers["Authorization"] = f"Bearer {req.api_key}"

        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{base_url}/models",
                headers=headers,
                timeout=10.0,
            )

            if response.status_code == 401:
                return TestConnectionResponse(success=False, error="Invalid API key")
            response.raise_for_status()

            data = response.json()

            # Handle both OpenAI format {"data": [...]} and alternative {"models": [...]}
            model_list = data.get("data") or data.get("models") or []

            # Extract model IDs - handle both dict and string formats
            models = []
            for m in model_list:
                if isinstance(m, dict):
                    model_id = m.get("id") or m.get("name")
                    if model_id:
                        models.append(model_id)
                elif isinstance(m, str):
                    models.append(m)

            return TestConnectionResponse(success=True, models=models)

    except httpx.ConnectError:
        return TestConnectionResponse(
            success=False,
            error=f"Cannot connect to server at {base_url}"
        )
    except httpx.TimeoutException:
        return TestConnectionResponse(
            success=False,
            error=f"Connection timed out - server at {base_url} may be slow or unreachable"
        )
    except Exception as e:
        return TestConnectionResponse(success=False, error=str(e))
```

3. Add test_openrouter endpoint immediately after:

```python
@app.post("/setup/test-openrouter", response_model=TestConnectionResponse)
async def test_openrouter(req: TestOpenRouterRequest) -> TestConnectionResponse:
    """Test OpenRouter API key and list available models with tool support.

    Args:
        req: TestOpenRouterRequest with API key

    Returns:
        TestConnectionResponse with success status and tool-capable model list
    """
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                "https://openrouter.ai/api/v1/models",
                params={"supported_parameters": "tools"},
                headers={"Authorization": f"Bearer {req.api_key}"},
                timeout=15.0,  # Longer timeout for large response
            )

            if response.status_code == 401:
                return TestConnectionResponse(success=False, error="Invalid API key")
            response.raise_for_status()

            data = response.json()
            models = [m.get("id") for m in data.get("data", [])]
            models = [m for m in models if m]

            return TestConnectionResponse(success=True, models=models)

    except httpx.TimeoutException:
        return TestConnectionResponse(
            success=False,
            error="Request timed out - try again"
        )
    except Exception as e:
        return TestConnectionResponse(success=False, error=str(e))
```

Key implementation notes:
- Use `base_url.rstrip("/")` to normalize URLs (avoid double slashes)
- Handle both `{"data": [...]}` and `{"models": [...]}` response formats for OpenAI-compatible
- Use `supported_parameters=tools` query param for OpenRouter to filter to tool-capable models
- 10s timeout for OpenAI-compatible, 15s for OpenRouter (larger response)
- Return models in TestConnectionResponse.models field (already exists in Pydantic model)
  </action>
  <verify>
```bash
cd /Users/mmaudet/work/CAAL && uv run ruff check src/caal/webhooks.py
cd /Users/mmaudet/work/CAAL && uv run mypy src/caal/webhooks.py --ignore-missing-imports
```
  </verify>
  <done>
- TestOpenAICompatibleRequest and TestOpenRouterRequest Pydantic models defined
- POST /setup/test-openai-compatible endpoint implemented with model discovery
- POST /setup/test-openrouter endpoint implemented with tool filtering
- Both endpoints reuse existing TestConnectionResponse model
- Lint and type check pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Add frontend proxy routes for new test endpoints</name>
  <files>
frontend/app/api/setup/test-openai-compatible/route.ts
frontend/app/api/setup/test-openrouter/route.ts
  </files>
  <action>
Create two new Next.js API route files following the exact pattern of test-groq/route.ts.

1. Create frontend/app/api/setup/test-openai-compatible/route.ts:

```typescript
import { NextRequest, NextResponse } from 'next/server';

const WEBHOOK_URL = process.env.WEBHOOK_URL || 'http://agent:8889';

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();

    const res = await fetch(`${WEBHOOK_URL}/setup/test-openai-compatible`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(body),
    });

    const data = await res.json();
    return NextResponse.json(data);
  } catch (error) {
    console.error('[/api/setup/test-openai-compatible] Error:', error);
    return NextResponse.json(
      { success: false, error: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}
```

2. Create frontend/app/api/setup/test-openrouter/route.ts:

```typescript
import { NextRequest, NextResponse } from 'next/server';

const WEBHOOK_URL = process.env.WEBHOOK_URL || 'http://agent:8889';

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();

    const res = await fetch(`${WEBHOOK_URL}/setup/test-openrouter`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(body),
    });

    const data = await res.json();
    return NextResponse.json(data);
  } catch (error) {
    console.error('[/api/setup/test-openrouter] Error:', error);
    return NextResponse.json(
      { success: false, error: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}
```

Both files are identical except for the endpoint path. This follows the existing pattern exactly.
  </action>
  <verify>
```bash
cd /Users/mmaudet/work/CAAL/frontend && pnpm lint
cd /Users/mmaudet/work/CAAL/frontend && pnpm format:check
```
  </verify>
  <done>
- frontend/app/api/setup/test-openai-compatible/route.ts exists with POST handler
- frontend/app/api/setup/test-openrouter/route.ts exists with POST handler
- Both proxy requests to WEBHOOK_URL with proper error handling
- Lint and format checks pass
  </done>
</task>

<task type="auto">
  <name>Task 3: End-to-end verification</name>
  <files></files>
  <action>
Verify the endpoints work correctly with real requests.

1. Start the agent webhook server if not running
2. Test OpenAI-compatible endpoint with a mock server or known endpoint
3. Test OpenRouter endpoint with invalid key (should return 401 error)
4. Verify frontend routes exist and return proper structure

Note: Full integration testing requires a running OpenAI-compatible server and valid OpenRouter API key. For verification, test error cases and response structure.
  </action>
  <verify>
```bash
# Verify endpoints are registered in webhooks.py
cd /Users/mmaudet/work/CAAL && grep -n "test-openai-compatible\|test-openrouter" src/caal/webhooks.py

# Verify frontend routes exist
ls -la /Users/mmaudet/work/CAAL/frontend/app/api/setup/test-openai-compatible/route.ts
ls -la /Users/mmaudet/work/CAAL/frontend/app/api/setup/test-openrouter/route.ts

# Verify route content contains WEBHOOK_URL pattern
grep -l "WEBHOOK_URL" /Users/mmaudet/work/CAAL/frontend/app/api/setup/test-*/route.ts
```
  </verify>
  <done>
- Both backend endpoints registered in webhooks.py
- Both frontend route files exist with correct proxy pattern
- Routes contain WEBHOOK_URL for proxying to agent
  </done>
</task>

</tasks>

<verification>
All phase checks:

1. Backend endpoints work:
   - `grep "test-openai-compatible" src/caal/webhooks.py` returns endpoint definition
   - `grep "test-openrouter" src/caal/webhooks.py` returns endpoint definition

2. Frontend routes work:
   - `ls frontend/app/api/setup/test-openai-compatible/route.ts` exists
   - `ls frontend/app/api/setup/test-openrouter/route.ts` exists

3. Code quality:
   - `uv run ruff check src/caal/webhooks.py` passes
   - `cd frontend && pnpm lint` passes

4. Response structure matches TestConnectionResponse:
   - `success: bool`
   - `error: str | None`
   - `models: list[str] | None`
</verification>

<success_criteria>
- POST /setup/test-openai-compatible endpoint accepts {base_url, api_key?} and returns {success, error?, models?}
- POST /setup/test-openrouter endpoint accepts {api_key} and returns {success, error?, models?}
- OpenAI-compatible endpoint handles both {"data": [...]} and {"models": [...]} response formats
- OpenRouter endpoint filters for tool-capable models using supported_parameters=tools
- Frontend routes /api/setup/test-openai-compatible and /api/setup/test-openrouter proxy to agent
- All lint and type checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-connection-testing-endpoints/10-01-SUMMARY.md`
</output>
